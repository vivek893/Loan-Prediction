{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17bc53e5",
   "metadata": {},
   "source": [
    "# Loan Prediction\n",
    "# - [Import Library](#Import-Library)\n",
    "# - [Load dataset](#Load-dataset)\n",
    "# - [Rename columns](#Rename-columns)\n",
    "# - [Display Row and Column](#Display-Row-and-Column)\n",
    "# - [Display Column_name](#Display-Column_name)\n",
    "# - [Display Null Columns](#Display-Null-Columns)\n",
    "# - [Remove Nan value](#Remove-Nan-value)\n",
    "# - [Delete Duplicates](#Delete-Duplicates)\n",
    "# - [Encode Categorical Data](#Encode-Categorical-Data)\n",
    "# - [Handle Outlier](#Handle-Outlier)\n",
    "# - [Binning](#Binning)\n",
    "# - [Normalization](#Normalization)\n",
    "# - [Visualisation](#Visualisation)\n",
    "# - [Line Plot](#Line-Plot)\n",
    "# - [Bar Plot](#Bar-Plot)\n",
    "# - [Histogram](#Histogram)\n",
    "# - [Box Plot](#Box-Plot)\n",
    "# - [Area Plot](#Area-Plot)\n",
    "# - [Scatter Plot](#Scatter-Plot)\n",
    "# - [Hexagonal Bin Plot](#Hexagonal-Bin-Plot)\n",
    "# - [Pie Chart](#Pie-Chart)\n",
    "# - [HeatMap](#HeatMap)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214167c0",
   "metadata": {},
   "source": [
    "# Import Library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd24ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report,ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c177f9e9",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data=pd.read_csv(\"Loan Application Accept or Reject.csv\")\n",
    "loan_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ad6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7612e6",
   "metadata": {},
   "source": [
    "# Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a4935",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.columns=['loan_id', 'no_of_dependents', 'education', 'employed',\n",
    "       'income_annum', 'loan_amount', 'loan_term', 'cibil_score',\n",
    "       'residential_assets_value', 'commercial_assets_value',\n",
    "       'luxury_assets_value', 'bank_asset_value', 'loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53296e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data['loan_status'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ab104",
   "metadata": {},
   "source": [
    "# Display Row and Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0442521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loan_data.shape)\n",
    "row,column=loan_data.shape\n",
    "print(\"total number of rows:\", row)\n",
    "print(\"total number of column:\", column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e695ac0",
   "metadata": {},
   "source": [
    " # Display Column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd5a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display column name, data type and size\n",
    "print(loan_data.columns)\n",
    "print(\"---------------XXXX------------\")\n",
    "print(loan_data.dtypes)\n",
    "print(\"---------------XXXX------------\")\n",
    "loan_data.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d63685e",
   "metadata": {},
   "source": [
    "# Display Null Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4189f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bae1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_nulls = loan_data[loan_data.isnull().any(axis=1)]\n",
    "print(rows_with_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b611d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show only nan columns name\n",
    "nan_column=loan_data.columns[loan_data.isnull().any()]\n",
    "for i in nan_column:\n",
    "    print(\"Columns name: \", i,\":\",loan_data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781699c7",
   "metadata": {},
   "source": [
    "# Remove Nan value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d26cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4fe412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the missing data before removing null value\n",
    "sns.heatmap(loan_data.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e60623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some data in dataset\n",
    "loan_data.loc[len(loan_data)]=[4270,2,'Not Graduate', 'Yes',np.nan,10000,10,300,100000.0,100000.0,np.nan,np.nan,'Approved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ca3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.loc[len(loan_data)]=[4271,2,np.nan, 'Yes',np.nan,10000,10,300,100000.0,100000.0,np.nan,np.nan,np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.loc[len(loan_data)]=[np.nan]*len(loan_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d86b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to delete data where all row value is nan\n",
    "loan_data.dropna(how='all',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe5e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data['loan_id']=loan_data['loan_id'].astype(int) # here we change because we add nan value. nan value is by-default float data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9af6c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete row from data set.\n",
    "loan_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan value in education column by using mean value of data\n",
    "# education data is string type so use mode. we use mean, median for numerical data type\n",
    "# here we used mode for imputy function \n",
    "loan_data['education'].fillna(loan_data['education'].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a5e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we delete two row.those row we added before \n",
    "loan_data=loan_data.drop(4269).reset_index(drop=True)\n",
    "loan_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8876fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_column=loan_data.columns[loan_data.isnull().any()]\n",
    "for i in nan_column:\n",
    "    print(\"Columns name: \", i,\":\",loan_data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc16e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use mean for imputing\n",
    "loan_data['income_annum'].fillna(loan_data['income_annum'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use median for imputing\n",
    "loan_data['residential_assets_value'].fillna(loan_data['residential_assets_value'].median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use dropna function to delete the remaining nan value\n",
    "loan_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c4938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the missing data after removing null value\n",
    "sns.heatmap(loan_data.isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41f9c4",
   "metadata": {},
   "source": [
    "# Delete Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2493f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Duplicates values from datasets\n",
    "loan_data[loan_data.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25074fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534bd67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc4186",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0892c2d",
   "metadata": {},
   "source": [
    "# Encode Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df=loan_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8abe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loan_data['education'].unique())\n",
    "print(loan_data['employed'].unique())\n",
    "print(loan_data['loan_status'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc145ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for education column we apply encoder\n",
    "education_encoder=LabelEncoder()\n",
    "loan_df['education']=education_encoder.fit_transform(loan_df['education'])\n",
    "# for employed column we apply encode\n",
    "employed_encoder=LabelEncoder()\n",
    "loan_df['employed']=employed_encoder.fit_transform(loan_df['employed'])\n",
    "# for loan_status column we apply encode \n",
    "loan_encoder=LabelEncoder()\n",
    "loan_df['loan_status']=loan_encoder.fit_transform(loan_df['loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bac826",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b34e8",
   "metadata": {},
   "source": [
    " # Handle Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12723670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for finding outlier on dataset we use select_dtypes method to \n",
    "numeric_cols=loan_df.select_dtypes(include=['number']).columns\n",
    "for col in numeric_cols:\n",
    "    q1=loan_df[col].quantile(0.25)\n",
    "    q3=loan_df[col].quantile(0.75)\n",
    "    IQR=q3-q1\n",
    "    print(\"IQR value : \",IQR)\n",
    "    lower_bound=q1-1.5*IQR\n",
    "    upper_bound=q3+1.5*IQR\n",
    "    outliers=loan_df[(loan_df[col]<lower_bound) | (loan_df[col]>upper_bound)]\n",
    "    outlier_row=outliers.any(axis=1)\n",
    "    print(\"Total outlier:\",outlier_row.sum())\n",
    "    print(f\"outlier in {col}: \",outliers.shape,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c7530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for finding outlier on dataset we use select_dtypes method to \n",
    "numeric_cols=loan_df.select_dtypes(include=['number']).columns\n",
    "for col in numeric_cols:\n",
    "    z = np.abs(stats.zscore(loan_df[col]))\n",
    "    outliers=loan_df[(abs(z)>3)]\n",
    "    outlier_row=outliers.any(axis=1)\n",
    "    print(\"Total outlier:\",outlier_row.sum())\n",
    "    print(f\"Outlier using z-szore: {col}:\",outliers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6378f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing outlier using box plot\n",
    "# Assuming `loan_df` is your DataFrame and `numeric_cols` contains numeric column names\n",
    "numeric_cols=loan_df.select_dtypes(include=['number']).columns\n",
    "# set number of columns per row\n",
    "plots_per_row=4\n",
    "#calculate number of rows needed\n",
    "num_rows=(len(numeric_cols)+plots_per_row-1)//plots_per_row\n",
    "# Create subplots\n",
    "fig,axes=plt.subplots(num_rows,plots_per_row,figsize=(5*plots_per_row,4*num_rows))\n",
    "# Flatten axes if it's a 2D array\n",
    "if len(numeric_cols) > 1:\n",
    "    axes = axes.flatten()\n",
    "else:\n",
    "    axes = [axes] # Ensure it's iterable for a single column\n",
    "# Loop through numeric columns and axes\n",
    "for ax,col in zip(axes,numeric_cols):\n",
    "    sns.boxplot(y=loan_df[col],ax=ax)\n",
    "    ax.set_title(f'Boxplot of {col} ')\n",
    "# remove unused subplots if any\n",
    "for i in range(len(numeric_cols),len(axes)):\n",
    "    fig.delaxes(axes[i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b469853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier using quertile\n",
    "# for finding outlier on dataset we use select_dtypes method to \n",
    "# outlier using quertile\n",
    "df=loan_df.copy()\n",
    "numeric_cols=df.select_dtypes(include=['number']).columns\n",
    "q1=df[numeric_cols].quantile(0.25)\n",
    "q3=df[numeric_cols].quantile(0.75)\n",
    "IQR=q3-q1\n",
    "outliers=((df[numeric_cols]<(q1-1.2*IQR)) |(df[numeric_cols]>(q3+1.2*IQR)))\n",
    "outlier_row=outliers.any(axis=1)\n",
    "print(f\"number of outliers detected:{outlier_row.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove outlier\n",
    "df_cleaned=df[~outlier_row].reset_index(drop=True)\n",
    "print(f\"original dataset shape: {df.shape}\")\n",
    "print(f\"cleaned dataset shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8790ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in numeric_cols:\n",
    "    fig,axes=plt.subplots(1,2,figsize=(12,4))\n",
    "    sns.boxplot(x=df[feature],ax=axes[0])\n",
    "    axes[0].set_title(f\"Before cleaning:{feature}\")\n",
    "    sns.boxplot(x=df_cleaned[feature],ax=axes[1])\n",
    "    axes[1].set_title(f\"After cleaning: {feature}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060e9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb3f41",
   "metadata": {},
   "source": [
    "# Binning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a40be2",
   "metadata": {},
   "source": [
    " Binning data is a common technique in data analysis where you group continuous data into categorical intervals, or bins, to gain insights into the distribution or trends within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of binning for cibil_score\n",
    "df_cleaned['cibil_score_bin']=pd.cut(df_cleaned['cibil_score'],bins=[300,600,700,850],labels=['Poor','Average','Good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5110a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binning loan_amount\n",
    "df_cleaned['loan_amount_bin']=pd.cut(df_cleaned['loan_amount'],bins=5)\n",
    "#Binning income_annum\n",
    "df_cleaned['income_bin']=pd.cut(df_cleaned['income_annum'],bins=3,labels=['low Salary','Medium Salary','High Salary'])\n",
    "#Binning loan_term\n",
    "df_cleaned['loan_term_bin']=pd.cut(df_cleaned['loan_term'],bins=[0,5,10,15,20,30],labels=['<5','5-10','10-15','15-20','20-30'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['loan_amount_bin'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006715ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cleaned['income_annum'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc35a3",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b28b6",
   "metadata": {},
   "source": [
    "Data normalization is the process of scaling numeric features to a standard range, preventing large values from dominating the learning process in machine learning models<br>\n",
    "The MinMaxScaler() function scales each feature to a given range, typically [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c42ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()\n",
    "df_cleaned[['loan_amount','loan_term','cibil_score','residential_assets_value','commercial_assets_value','luxury_assets_value','bank_asset_value','income_annum']]=scaler.fit_transform(df_cleaned[['loan_amount','loan_term','cibil_score','residential_assets_value','commercial_assets_value','luxury_assets_value','bank_asset_value','income_annum']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0749eda",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b77608",
   "metadata": {},
   "source": [
    "# Line Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a22b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plot: line plot\n",
    "df_sorted=df_cleaned.sort_values('loan_term')\n",
    "plt.plot(df_sorted['loan_term'])\n",
    "plt.title(\"loan Term trend\")\n",
    "plt.xlabel(\"Applicants\")\n",
    "plt.ylabel(\"Loan Term\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bce93f",
   "metadata": {},
   "source": [
    "Inference:<br>\n",
    "It basically show all sorted loan Applicants on the basis of loan Term. It basically show more people intrested in taking loan for upto 4 to 6 years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7010f09",
   "metadata": {},
   "source": [
    "# Bar Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723c9777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bar plot\n",
    "ax=sns.countplot(x='education',hue='loan_status',data=df_cleaned)\n",
    "for p in ax.patches:\n",
    "    height=p.get_height()\n",
    "    ax.annotate(f'{height}',(p.get_x()+p.get_width()/2.,height),ha='center',va='bottom')\n",
    "plt.title('Loan Approval by education')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b687c68b",
   "metadata": {},
   "source": [
    "Inference:-<br>\n",
    "According to bank policies graduates have higher chance to approval loan by bank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='loan_status',data=df_cleaned)\n",
    "plt.title(\"loan Approval Distribution\")\n",
    "plt.xlabel(\"Approval\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262144a",
   "metadata": {},
   "source": [
    "Inference:- <br>\n",
    "The Majority of application are rejected, with a lesser number Approved. This shows the bank's rejection rate is higher than its Approval rate rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81608d",
   "metadata": {},
   "source": [
    "# Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a05126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram\n",
    "df['income_annum'].hist(bins=40)\n",
    "plt.title(\"Distribution of Annual income\")\n",
    "plt.xlabel(\"Income\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08319be8",
   "metadata": {},
   "source": [
    "Inference:-<br>\n",
    "The Distribution of annual income is right skewed, meaning most people earn less, with a few high earner. This might indicated income inequality within the application pool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d297be3",
   "metadata": {},
   "source": [
    " Skewness is a key statistical measure that shows how data is spread out in a dataset. It tells us if the data points are skewed to the left (negative skew) or to the right (positive skew) in relation to the mean. It is important because it helps us to understand the shape of the data distribution which is important for accurate data analysis and helps in identifying outliers and finding the best statistical methods to use for analysis.\n",
    "1. Positive Skewness (Right Skew)\n",
    "In a positively skewed distribution, the right tail is longer than the left which means most data points are on the left with a few large values pulling the distribution to the right.\n",
    "Relationship:\n",
    "Mean > Median > Mode\n",
    "2. Negative Skewness (Left Skew)\n",
    "In a negatively skewed distribution, the left tail is longer which means most data points are on the right with a few smaller values pulling the distribution to the left.\n",
    "Relationship:\n",
    "Mean < Median < Mode\n",
    "3. Zero Skewness (Symmetrical Distribution)\n",
    "Zero skewness shows a perfectly symmetrical distribution where the mean, median and mode are equal. In a symmetrical distribution, the data points are evenly distributed around the central point.\n",
    "Relationship:\n",
    "Mean = Median = Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b74f30f",
   "metadata": {},
   "source": [
    "\n",
    "# Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53272296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Box plot\n",
    "sns.boxplot(x='loan_status',y='loan_amount',data=df_cleaned)\n",
    "plt.title(\"loan Amount by Approval\")\n",
    "plt.xlabel(\"Approval\")\n",
    "plt.ylabel(\"loan Approval (Normalization)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d91b6c9",
   "metadata": {},
   "source": [
    "Inference:-<br>\n",
    "The box plot shows that the median loan amount is higher for approval loans. there are a few outliers with very large amount, which might corresspond to special cases or high-risk application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47455da",
   "metadata": {},
   "source": [
    "# Area Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a65804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Area Plot\n",
    "df_cleaned[['income_annum','loan_amount']].sort_values(by='income_annum').plot.area()\n",
    "plt.title(\"Area plot of Annual income and loan Amount\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3f85a",
   "metadata": {},
   "source": [
    " Inference:- <br>\n",
    "This Area plot illurstrate the cummulative distribution of Annual Income and loan amount. we observe a clear upward trend-higher income typically comes alongside larger loans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38610fd",
   "metadata": {},
   "source": [
    "# Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e10fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter Plot\n",
    "sns.scatterplot(x='income_annum',y='loan_amount',hue='loan_status',data=df_cleaned)\n",
    "plt.title(\"Scatter plot of Annual income vs loan Amount by Approval\")\n",
    "plt.xlabel(\"Annual Income\")\n",
    "plt.ylabel(\"Loan Amount\")\n",
    "plt.legend(title='Approval')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c1452",
   "metadata": {},
   "source": [
    "Inference:-<br>\n",
    "This scatter plot shows a positive correlation between annual income and loan amount.Approved application are predominantly in  the higher income and higher amount quadrant, while non-approval ones are more dispersed at lower values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21859f5",
   "metadata": {},
   "source": [
    "\n",
    "# Hexagonal Bin Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b6d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hexbin Plot\n",
    "df_cleaned.plot.hexbin(x='income_annum',y='loan_amount',gridsize=25,cmap='Blues')\n",
    "plt.title(\"Hexbin plot of annual income vs loan amount\")\n",
    "plt.xlabel(\"Annual income\")\n",
    "plt.ylabel(\"Loan Amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e263ff6",
   "metadata": {},
   "source": [
    "Inference:-<br>\n",
    " This hexbin plot highlights the connection of data point. The densest cluster are the lower income and lower loan which resonattes with our observation from the scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c131f",
   "metadata": {},
   "source": [
    "# Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee04fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "explode=[0.1,0,0]\n",
    "df_cleaned['income_bin'].value_counts().plot.pie(autopct='%1.1f%%',counterclock=False,startangle=50,explode=explode)\n",
    "plt.title(\"Annual income distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944120a2",
   "metadata": {},
   "source": [
    "inference:-<br>\n",
    "The Pie chart shows the residential assets value which are divided into three categories of high, low and medium. More people earn medium salary and lower salary. fewer people lies in higher salary range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cfafc5",
   "metadata": {},
   "source": [
    "# HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06247231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df_cleaned.corr(numeric_only=True),annot=True,cmap=sns.cubehelix_palette(as_cmap=True))\n",
    "plt.title(\"Correlation heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92097cb",
   "metadata": {},
   "source": [
    "Inference:-<br>\n",
    "According to the correlation graph, loan status is significantly influenced by factors such as education level, employemnt status, and residential asset value. Individual with higher annual income also tend to own more residential, commerical and luxury assets. Additionally, the loan amount is strongly correlated with annual income, indicating that higher earners are eligible for longer loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd70e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for education column we apply encoder\n",
    "income_encoder=LabelEncoder()\n",
    "df_cleaned['income_bin']=income_encoder.fit_transform(df_cleaned['income_bin'])\n",
    "# for employed column we apply encode\n",
    "cibil_encoder=LabelEncoder()\n",
    "df_cleaned['cibil_score_bin']=cibil_encoder.fit_transform(df_cleaned['cibil_score_bin'])\n",
    "loan_amount_encoder=LabelEncoder()\n",
    "df_cleaned['loan_amount_bin']=loan_amount_encoder.fit_transform(df_cleaned['loan_amount_bin'])\n",
    "loan_term_encoder=LabelEncoder()\n",
    "df_cleaned['loan_term_bin']=loan_term_encoder.fit_transform(df_cleaned['loan_term_bin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e639629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3962228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df_cleaned.drop(['loan_id','cibil_score_bin','loan_amount_bin','income_bin','loan_term_bin'],axis=1)\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2a8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "x=df_cleaned.drop(columns=['loan_status'])\n",
    "y=df_cleaned['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866bbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78068bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "model=LogisticRegression()\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae72d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on X_test(from train_test_split)\n",
    "joblib.dump(model,\"load_model.pkl\")\n",
    "print(\"model saved as loan_model.pkl\")\n",
    "y_pred=model.predict(x_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f72295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show Confusion Matrix\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "\n",
    "disp=ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "print(\"Confusion Matrix\",cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8103ea",
   "metadata": {},
   "source": [
    "Inference:<br>\n",
    "total True negative is 467<br>\n",
    "total false positive is 39<br>\n",
    "total false negative is 16 <br>\n",
    "total positive is 269<br>\n",
    "\n",
    "Confusion Matrix:\n",
    "A confusion matrix is a table used to evaluate the performance of classification model especially in binary and multi-class classification <br>\n",
    "                 predicted<br>\n",
    "                  0      1 <br>\n",
    " Actual value  0  TN     FP<br>\n",
    "               1  FP    TP <br>\n",
    " TP (True Positive): model predicted 1(positive and actual is also 1)<br>\n",
    " TN ( True Negative): Model predicted 0(negative) and actual is also 0)<br>\n",
    " FP ( False Positive): Model predicted 1 but actual is 0( type | error)<br>\n",
    " FN ( False Negative): Model predicted 0 but acutal is 1 ( type| error)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7202d7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show Accuracy, score and classification report\n",
    "print(\"Accuracy Score:\",accuracy_score(y_test,y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test,y_pred)*100:.2f}%\")\n",
    "print(\"Classification Report:\\n\",classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ffc9f6",
   "metadata": {},
   "source": [
    "Classification Report:<br>\n",
    "A classification report summarizes the performance of a classification model using key metrics for each class. <br>\n",
    "Precision: out of all predicted positive , how many were actually correct (TP/(TP+FP))<br>\n",
    "Recall: out of all actual positives, how many did the model correct identity?(TP/(TP+FN)) <br>\n",
    " F1-score: Balance between precision and recall (2*(precision * recall)/(precision+recall)) <br>\n",
    "support: Number of actual instance for each class in the test set <br>\n",
    "Accuracy: (TP+TN)/Total overall how many prediction were correct<br>\n",
    "Macro Avg: Average of precision , recall, f1-score across all classess equally. <br>\n",
    "weighted Avg: Average of precision , recall , f1-score weighted by support(sample count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1fcdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Graph: Actual vs prediction\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(list(range(len(y_test))),y_test.values,marker='o',label=\"Actual\")\n",
    "plt.plot(list(range(len(y_pred))),y_pred,marker='X',label=\"Predicted\",linestyle='dashed')\n",
    "plt.title(\"Actual vs predicted loan status\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"loan Status (0=Reject,1=Approved)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For User:\n",
    "def predict_loan_status(user_input):\n",
    "    user_df=pd.DataFrame([user_input])\n",
    "    prediction=model.predict(user_df)[0]\n",
    "    return \"Loan Approved\" if prediction==1 else \"Loan Rejected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d210aac",
   "metadata": {},
   "source": [
    "# for User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c629005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take user input:\n",
    "user_input={\n",
    "    'no_of_dependents':int(input(\"Enter your number of dependents: \")),\n",
    "    'education':int(input(\"Enter your education status(if Graduate then enter 1 otherwise 0): \")),\n",
    "    'employed':int(input(\"Enter your employed status(if employed press 1 otherwise 0): \")),\n",
    "    'income_annum':int(input(\"Enter your per annum income: \")),\n",
    "    'loan_amount':int(input(\"Enter your loan amount: \")),\n",
    "    'loan_term':int(input(\"Enter your loan term: \")),\n",
    "    'cibil_score':int(input(\"Enter your cibil score(300-1000): \")),\n",
    "    'residential_assets_value':int(input(\"Enter your Residential assets: \")),\n",
    "    'commercial_assets_value':int(input(\"Enter your commercial assets value: \")),\n",
    "    'luxury_assets_value':int(input(\"Enter your luxury assets value: \")),\n",
    "    'bank_asset_value':int(input(\"Enter your bank assets value: \")),\n",
    "\n",
    "\n",
    "}\n",
    "print(predict_loan_status(user_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8cf7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf095c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c957e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb7d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
